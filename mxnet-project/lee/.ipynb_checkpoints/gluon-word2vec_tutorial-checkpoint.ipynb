{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipgram Model using MXNet/Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorials aims to teach how to implement word2vec skipgram model(negative sampling) with gluon interface.  \n",
    "However note that python is not suitable for training word2vec model, data iterator in mxnet is limited to be single threaded and   additional trick is required for asynchronous SGD. This tutorial is for demonstration purpose only.  \n",
    "\n",
    "## Other resources\n",
    "\n",
    "For high performance library refer to  \n",
    "\n",
    " - cython version [Genism](https://radimrehurek.com/gensim/models/word2vec.html), \n",
    " - or Original C version [Google](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "## CBOW version\n",
    "\n",
    "Peopel who are interested in cbow model of word2vec(nce loss), refer to  \n",
    "[CBOW](https://github.com/apache/incubator-mxnet/blob/master/example/nce-loss/wordvec.py)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You will acquire the following:\n",
    "\n",
    "- explain what skipgram model is\n",
    "- implement custom blocks with Gluon Inteface, \n",
    "- extract, save and load parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this tutorial, you need:\n",
    "\n",
    "- [MXNet/Gluon](http://mxnet.io/get_started/setup.html#overview)\n",
    "- [Language](https://www.python.org/)\n",
    "- [Embedding Benchmark](https://github.com/kudkudak/word-embeddings-benchmarks)\n",
    "- Familiarity with linear algebra, simple multi layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys, random, time, math\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Block, nn\n",
    "from mxnet import autograd\n",
    "import _pickle as cPickle\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[text8](http://mattmahoney.net/dc/textdata.html) **wikipedia dump**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  local_filename = os.path.join(gettempdir(), filename)\n",
    "  if not os.path.exists(local_filename):\n",
    "    local_filename, _ = urllib.request.urlretrieve(url + filename,\n",
    "                                                   local_filename)\n",
    "  statinfo = os.stat(local_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception('Failed to verify ' + local_filename +\n",
    "                    '. Can you get to it with a browser?')\n",
    "  return local_filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split into list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words.\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = str(f.read(f.namelist()[0]))\n",
    "  return data\n",
    "buf = read_data(filename)\n",
    "vocabulary = buf.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    dictionary = {}\n",
    "    reverse_dictionary = [\"NA\"]\n",
    "    count = [0] \n",
    "    data = [] \n",
    "    for word in vocabulary:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if word not in dictionary:\n",
    "            dictionary[word] = len(dictionary) + 1\n",
    "            count.append(0)\n",
    "            reverse_dictionary.append(word)\n",
    "        wid = dictionary[word]\n",
    "        data.append(wid)\n",
    "        count[wid] += 1\n",
    "    negative = [] \n",
    "    for i, v in enumerate(count):\n",
    "        if i == 0 or v < 5:\n",
    "            continue\n",
    "        v = int(math.pow(v * 1.0, 0.75))\n",
    "        negative += [i for _ in range(v)]\n",
    "    return data, count, dictionary, reverse_dictionary, negative\n",
    "vocabulary_size = 50000\n",
    "data, count, dictionary, reverse_dictionary, negative = build_dataset(vocabulary, vocabulary_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the raw text, we will build the followings\n",
    "\n",
    "#### Dictionary\n",
    "dictionary['apple'] returns integer index\n",
    "#### Reverse dictionary \n",
    "reverse_dictionary['21'] returns corresponding word\n",
    "#### Count\n",
    "count['apple'] returns the occurences of apple in the corpus\n",
    "#### Negative\n",
    "this table will be used for sampling words in the training phase\n",
    "#### Data\n",
    "sequence of word is converted to sequence of indices, using dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Data Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataBatch(object):\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "class Word2VecDataIterator(mx.io.DataIter):\n",
    "    def __init__(self, batch_size=512, num_neg_samples=5, window=5):\n",
    "        super(Word2VecDataIterator, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.negative_samples = num_neg_samples\n",
    "        self.window = window\n",
    "        self.data, self.negative, self.dictionary = (data, negative, dictionary)\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [('contexts', (self.batch_size, 1))]\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return [('targets', (self.batch_size, self.negative + 1))]\n",
    "\n",
    "    def sample_ne(self):\n",
    "        return self.negative[random.randint(0, len(self.negative) - 1)]\n",
    "\n",
    "    def __iter__(self):\n",
    "        input_data = []\n",
    "        update_targets = []\n",
    "        for pos, word in enumerate(self.data):\n",
    "            for index in range(-self.window, self.window + 1):\n",
    "                if (index != 0 and pos + index >= 0 and pos + index < len(self.data)):\n",
    "                    context = self.data[pos + index]\n",
    "                    if word != context:\n",
    "                        input_data.append([word])\n",
    "                        targets = []\n",
    "                        targets.append(context) # positive sample\n",
    "                        while len(targets) < self.negative_samples + 1: # negative sample\n",
    "                            w = self.sample_ne()\n",
    "                            if w != word:\n",
    "                                targets.append(w)\n",
    "                        update_targets.append(targets)\n",
    "\n",
    "            # Check if batch size is full\n",
    "            if len(input_data) > self.batch_size:\n",
    "                batch_inputs = [mx.nd.array(input_data[:self.batch_size])]\n",
    "                batch_update_targets = [mx.nd.array(update_targets[:self.batch_size])]\n",
    "                yield DataBatch(batch_inputs, batch_update_targets)\n",
    "                input_data = input_data[self.batch_size:]\n",
    "                update_targets = update_targets[self.batch_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training-data](http://mccormickml.com/assets/word2vec/training_data.png)\n",
    "## For example\n",
    "if the size of the vocabulary is 10,  \n",
    "then the size of the input and output layer is 10.  \n",
    "(size, meaning number of unit in a layer)  \n",
    "output will be something like this (0, 0.1, 0.7, 0.1, 0.1, 0, 0, 0, 0, 0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why 1 Hot Encoding?\n",
    "1 hot encoded input will select only the corresponding row from the input weight matrix.  \n",
    "**note** input weight matrix connects input and the hidden layer, this matrix is the vector we are trying to learn. (= word vectors)   \n",
    "\n",
    "### Why Negative Sampling? \n",
    "At each training phase, we will not be calculating softmax loss over entire vocabulary.  \n",
    "we will designate specific units to perform update on.  \n",
    "thus a training sample will consists of context word that actually appears and five others that doesn't.   \n",
    "negative samples are selected from the negative table we have built previously.  \n",
    "\n",
    "e.g.)\n",
    "\"the quick\"  \n",
    "  \n",
    "center : quick  \n",
    "context : the  \n",
    "  \n",
    "postive sample (quick,the)  \n",
    "negative sample (quick, fox)  \n",
    "negative sample (quick, jump)  \n",
    "negative sample (quick, cat)  \n",
    "negative sample (quick, runs)  \n",
    "negative sample (quick, of)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary_size = len(reverse_dictionary)\n",
    "batch_size = 512\n",
    "num_hiddens = 100\n",
    "num_negative_samples = 5\n",
    "\n",
    "ctx = mx.gpu()\n",
    "data_iterator = Word2VecDataIterator(batch_size=batch_size,\n",
    "                                     num_neg_samples=num_negative_samples,\n",
    "                                     window=5)\n",
    "\n",
    "for batch in data_iterator:\n",
    "    batches.append(batch)\n",
    "    if (counting % 500 == 0):\n",
    "        print(counting)\n",
    "    counting = counting + 1\n",
    "\n",
    "    cPickle.dump(training_data, open('all_batches.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(gluon.HybridBlock):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Model, self).__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.center = nn.Embedding(input_dim=dictionary_size,\n",
    "                                       output_dim=num_hiddens,\n",
    "                                       weight_initializer=mx.initializer.Uniform(1.0 / num_hiddens))\n",
    "\n",
    "            self.target = nn.Embedding(input_dim=dictionary_size,\n",
    "                                       output_dim=num_hiddens,\n",
    "                                       weight_initializer=mx.initializer.Zero())\n",
    "\n",
    "    def hybrid_forward(self, F, center, targets, labels):\n",
    "        input_vectors = self.center(center)\n",
    "        update_targets = self.target(targets)\n",
    "        predictions = F.broadcast_mul(input_vectors, update_targets)\n",
    "        predictions = F.sum(data=predictions, axis=2)\n",
    "        sigmoid = F.sigmoid(predictions)\n",
    "        loss = F.sum(labels * F.log(sigmoid) + (1 - labels) * F.log(1 - sigmoid), axis=1)\n",
    "        loss = loss * -1.0 / batch_size\n",
    "        loss_layer = F.MakeLoss(loss)\n",
    "        return loss_layer\n",
    "model = Model()\n",
    "model.initialize(ctx=ctx)\n",
    "model.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skipgram-architecture](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "## Architecture\n",
    "given a word, cbow and skipgram model both tries to predict a word.  \n",
    "for skipgram the input is the **center** word, and output is the probability of a **context** word.  \n",
    "input layer takes a single word(1 hot encoded) and output layer outputs probability of every word appearing in the context. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Center embedding\n",
    "from the 1hot encoded input, we will be selecting word vectors from the weight matrix\n",
    "only a single row will be selected\n",
    "  \n",
    "### Target embedding\n",
    "from the target (postive sample, negative sample1,  negative sample2,  negative sample3,  negative sample4,  negative sample5)\n",
    "only the 6 row will be selected\n",
    "  \n",
    "### Calculation\n",
    "\n",
    "The skipgram architecture tries to predict the context given a word. The problem of predicting context words is framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. For the word at position $t$ we consider all context words as positive examples and sample negatives at random from the dictionary. For a chosen context position $c$, using the binary logistic loss, we obtain the following negative log-likelihood:\n",
    "\n",
    "$$ \\log (1 + e^{-s(w_t, w_c)}) +  \\sum_{n \\in \\mathcal{N}_{t,c}}^{}{\\log (1 + e^{s(w_t, n)})}$$\n",
    "\n",
    "where $w_t$ is a center word, $w_c$ is a context word, $\\mathcal{N}_{t,c}$ is a set of negative examples sampled from the vocabulary. By denoting the logistic loss function $l : x \\mapsto \\log(1 + e^{-x})$, we can re-write the objective as:\n",
    "\n",
    "$$ \\sum_{t=1}^{T}{ \\sum_{c \\in C_t}^{}{ \\big[ l(s(w_t, w_c))} + \\sum_{n \\in \\mathcal{N}_{t,c}}^{}{l(-s(w_t, n))}   \\big]} $$\n",
    "\n",
    "where $s(w_t, w_c) = u_{w_t}^T v_{w_c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {'learning_rate': 4, 'clip_gradient': 5})\n",
    "\n",
    "labels = nd.zeros((batch_size, num_negative_samples + 1), ctx=ctx)\n",
    "labels[:, 0] = 1 # [1 0 0 0 0 0]\n",
    "start_time = time.time()\n",
    "num_epochs = 10\n",
    "\n",
    "def get_loss(epoch_n, batch_n, loss):\n",
    "    if(batch_n==0 and epoch_n==0):\n",
    "        loss = loss.asnumpy().sum()\n",
    "    else:\n",
    "        loss = .99 * loss + .01 * loss.asnumpy().sum()\n",
    "    if(i + 1) % 100 == 0:\n",
    "        print(\"%sth epoch , %sth batch. avg of loss: %s\" % (epoch_n, batch_n, loss))\n",
    "    return loss\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    moving_loss = 0.\n",
    "    for i, batch in enumerate(batches):\n",
    "        center_words = batch.data[0].as_in_context(ctx)\n",
    "        target_words = batch.label[0].as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            loss = model(center_words, target_words, labels)\n",
    "        loss.backward()\n",
    "        # ignore_stale_grad ; only update calculated target weights\n",
    "        trainer.step(1, ignore_stale_grad=True)\n",
    "        moving_loss = get_loss(e, i, moving_loss)\n",
    "        if i > 15000:\n",
    "            break\n",
    "    print(\"1 epoch took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use large batch size 512, update weights by sgd.  \n",
    "**note** original c version of w2vec uses batch size 1, asynchronous sgd.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# format index : vector\n",
    "key = list(model.collect_params().keys())\n",
    "all_vecs = model.collect_params()[key[0]].data().asnumpy()\n",
    "cPickle.dump(all_vecs, open('all_vecs.p', 'wb'))\n",
    "\n",
    "#  foramt word : vector\n",
    "w2vec_dict = dictionary.copy()\n",
    "for word in dictionary:\n",
    "    idx = dictionary[word]\n",
    "    vector = all_vecs[idx]\n",
    "    w2vec_dict[word] = vector\n",
    "\n",
    "cPickle.dump(w2vec_dict, open('w2vec_dict.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "num_points = 450\n",
    "\n",
    "tsne = TSNE(perplexity=50, n_components=2, init='pca', n_iter=10000)\n",
    "two_d_embeddings = tsne.fit_transform(all_vecs[:num_points])\n",
    "labels = index_to_word[:num_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def plot(embeddings, labels):\n",
    "  assert embeddings.shape[0] >= len(labels), 'More labels than embeddings'\n",
    "  pylab.figure(figsize=(20,20))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = embeddings[i,:]\n",
    "    pylab.scatter(x, y)\n",
    "    pylab.annotate(label, xy=(x, y), xytext=(5, 2), textcoords='offset points',\n",
    "                   ha='right', va='bottom')\n",
    "  pylab.show()\n",
    "\n",
    "plot(two_d_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "(optional) word embediing bench mark  \n",
    "[Embedding Benchmark](https://github.com/kudkudak/word-embeddings-benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from web.datasets.analogy import fetch_google_analogy\n",
    "from web.embeddings import fetch_SG_GoogleNews\n",
    "import numpy as np\n",
    "import _pickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = load_embedding(\"w2vec_dict.p\", format=\"dict\")\n",
    "data = fetch_google_analogy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analogy question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo task1\n",
    "subset = [1, 200, 1000, 3000, 5500]\n",
    "for id in subset:\n",
    "    w1, w2, w3 = data.X[id][0], data.X[id][1], data.X[id][2]\n",
    "    print(\"Question: {} is to {} as {} is to ?\".format(w1, w2, w3))\n",
    "    print(\"Answer: \" + data.y[id])\n",
    "    print(\"Predicted: \" + \" \".join(w.nearest_neighbors(w[w2] - w[w1] + w[w3], exclude=[w1, w2, w3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 nearest word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo task2\n",
    "targets = [\"south\", \"usa\", \"eight\", \"earth\"]\n",
    "print(\"top 5 similar words list\")\n",
    "for target in targets:\n",
    "    print(\"'\"+target+\"': \"+str(w.nearest_neighbors(w[target], k = 5,  exclude = [target])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cosine similarity between two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# demo task3\n",
    "print(\"cosine similarity measure\")\n",
    "word_pairs = [(\"france\", \"spain\"),(\"newton\", \"apple\"),(\"coke\", \"hamburger\")]\n",
    "for pair in word_pairs :\n",
    "    cosine_similarity = np.dot(w[pair[0]], w[pair[1]])/(np.linalg.norm(w[pair[0]])* np.linalg.norm(w[pair[1]]))\n",
    "    print(\"similarity score {} , {} : {} \".format(pair[0], pair[1] ,cosine_similarity))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
